# Regularization Techniques
![enter image description here](https://github.com/ArijitChakrabarti/Deep-Learning-MNIST-Arijit/blob/main/1_fN4NFVVzaLs1QHezRtWKGA.jpeg?raw=true)

Artificial neural networks are prone to overfitting.  It is however interesting to note that there are many methods to control this level of overfitting.
-

This github project identifies three regularisation techniques for neural networks namely

- L1 regularisation
- L2 regularisation
- Dropout &
- Early Stopping

The datasets on which this python code was created can be downloaded by clicking [here.](https://www.kaggle.com/oddrationale/mnist-in-csv)
Please save the files locally on your system and then run the code from the ipynb file - which can be had by clicking [here](https://github.com/ArijitChakrabarti/Deep-Learning-MNIST-Arijit/blob/main/Deep_Learning_Regularisation_techniques.ipynb) on Google Colab. Google Colab is what I used to create this file however feel free to use any other notebook that your heart desires.

