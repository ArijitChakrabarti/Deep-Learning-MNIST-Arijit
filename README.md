# Regularization Techniques
![enter image description here](https://github.com/ArijitChakrabarti/Deep-Learning-MNIST-Arijit/blob/main/1_fN4NFVVzaLs1QHezRtWKGA.jpeg?raw=true)

Artificial neural networks are prone to overfitting.  It is however interesting to note that there are many methods to control this level of overfitting.
-

This github project identifies three regularisation techniques for neural networks namely

- L1 regularisation - uses a cost function  + a Regularisation term
- L2 regularisation - used lambda as the regularisation parameter, better known as weight decay i.e. tending to zero but not exactly 0
- Dropout  - randomly basis parameters selectes and removes nodes; effectively an ensemble technique in machine learning
- Early Stopping - cross-validation strategy to stop training once performance on validation set no longer improves

The datasets on which this python code was created can be downloaded by clicking [here.](https://www.kaggle.com/oddrationale/mnist-in-csv)
Please save the files locally on your system and then run the code from the ipynb file - which can be had by clicking [here](https://github.com/ArijitChakrabarti/Deep-Learning-MNIST-Arijit/blob/main/Deep_Learning_Regularisation_techniques.ipynb) on Google Colab. Google Colab is what I used to create this file however feel free to use any other notebook that your heart desires.

